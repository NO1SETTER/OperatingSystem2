181860152 周宇翔L1 实验报告

1.框架
本实验的框架为
定义Block数据结构管理每一块分配出去的内存空间
用两条并发链表管理共享内存部分的分配和回收,空链表头分别为
free_head，alloc_head,其中free_head按照严格升序的方式链接每一个未分配的管理块，
而alloc_head中管理的已分配块之间可以随意链接。

我将把可用的内存空间[_heap.start,_heap.end)划分成为三块
[_heap.end-0x2000000,_heap.end)为专门用于存放Block结构的区域
Block结构的申请释放，我另外用balloc，bfree函数进行管理，由于Block结构大小确定
这一部分只需要用类似栈的方式往上垒就很好实现

[_heap.end-0x2000000-0x800000*_ncpu(),_heap.end-0x2000000)为slab的区域，我给
每个CPU分配0x2000000的大小空间用作它专用的slab空间

[_heap.start,_heap.end-0x2000000-0x800000*_ncpu())则是本实验用kfree和kalloc主要管理的内存区域

kalloc的实现思路为
开始时先调用slab_kalloc,slab_kalloc和kalloc的区别仅在于无锁且分配的内存范围不同.
如果slab_kalloc成功返回指针则直接返回.
否则从全局链表free_head中依次查找有无能分配该内存的区间

kfree则是先从当前CPU对应的slab中去找有无对应的区块，如果没有再到alloc_head所管理的共享
内存中去找，如果找到了则将该Block从alloc_head中删除,并插入到free_head中去
这里的链表insert设计了两种模式，一种是普通的插入，另一种是检测前后节点代表的区间是否相连，如果相连就直接合并，这很符合本实验的要求

并发部分用了三把大锁实现,因为fine-grained lock实在是过于难实现
glb_lock用于控制两条全局链表的分配
alloc_lock用于控制Block结构的分配释放
print_lock用于保证输出信息的完整性，实际可以不用用到

2.bug?
遇到的bug其实没有什么让人顿悟或者让人感觉醍醐灌顶的那种，但是因为并发程序会无限放大一个小错误，然后又极其难调试，有时你很难发现到底是程序的本身写错了还是并发部分写的不够好，所以很容易就是数组越界之类的错误让人十分困扰(谁能想到我被这个困扰了半天呢).但是由并发本身导致的错误倒是没有那么地难调，比如AA型死锁，基本上把程序逻辑理一理，多打几个断点就看出来了.(当然我这么说可能是因为我就完全没有感觉到现在程序中有哪些埋得很深的错误)

3.测试
提交后Hard Test过了一个，但是好像是最复杂的那个，就很奇怪...在本地上设计了四种workloads，分别是完全随机，大小交替，超频繁小，频繁大，同时写了两个函数用于检查全局链表alloc_head和free_head的合法性，每次分配释放后都会进行检查，也没有出现什么问题，这样就就感觉debug无从下手了，所以就只能这样了.如果还有问题应该会在之后的试验中暴露出来

--------------------------------------

181860152 周宇翔L2　实验报告 7.7提交版
之前提交的L1报告文件名打成18186015.pdf了，如果没有找到实验报告可以找下这个文件名.
这次提交在OJ上只能过easy tests而不能过hard test，但在本地测试运行并没有出现任何问题,所以很难debug,如果OJ成绩是以历史最高分为准，请以6月7日10:07提交版为参考，该版本通过了全部easy test和一个hard test.但这次的提交实现更为标准，正确和精致，我总感觉自己离正确很近了，但是还差一口气


对于L2的感想:
1.L2的代码量比L1小很多，但是理解上的难度要稍微大一些,首先是kmt模块的实现,这一块还好,一方面中断机>制的实现可以通过阅读abstract machine中的cte.c来辅助理解,另一方面可以参考上学期的ics实现来做，只要做到合理地分配和设置就能完成.另外，由于之前M2中做协程管理的经验，这里管理线程也可以采用类似的方式
实现并套用API.
另一个则是信号量的实现，这个看起来原理挺简单，其实有挺多细节的，比如开中断和关中断的时机要把握好，哪一部分需要上锁也要斟酌。并且还要设计相应的数据结构来管理信号量的waiter,并提供唤醒waiter和使线程变为waiter的接口.

2.比较大的bug还是那个大家都卡住的地方，也即在不使用CPU绑定的情况下，如何防止进入os_trap的线程在返回前被别的处理器调用。我对这个bug有些后知后觉导致在debug上做了很多无用功。这里采用的思路是设置一个标志位is_trap标记进入os_trap的线程，使它不能被调度，并在下一次进入os_trap时更新之前线程的标志位。这一部分需要搞清楚哪一部分是需要利用is_trap来保护的,而非简单的在进入os_trap时设置，细节比较多。这样的实现比较简单，可以有效避免之前的情况

3.调度策略是round-robin，但是在处理器数接近线程数时，可能会出现一部分线程因为信号量的关系被阻塞，另一部分线程自陷入os_trap后由于设置了is_trap也不能被调度,所有处理器都会被卡住.这里做了一个改进就是在schedule转了足够久而没有找到合适线程后，如果自陷进来的线程是可以继续运行的，我们允许在保持它的is_trap情况下选取它继续运行，事实证明这样做效果很好，在9线程8处理器的情况下也能保持正常运行

4.为了保证线程运行比较均匀，我们用ct记录每个线程被schedule的次数来模拟它们运行的时间，在sem_signal的时候，我们从waiterlist中选取ct最小的线程进行唤醒来保持均匀。这样保持均匀运行的效果也不错，在一般情况下，各个线程彼此之间被调度的次数相差不超过5%

5.spinlock中中断的管理参考xv6代码的实现，基本上是照搬。在OJ上提交的问题也出在中断这一块，大概问题就是解锁的次数会比上锁的次数多，导致记录的中断层数某时会小于0，但是本地哪怕smp=8都没问题就很头疼。另外我也想象不到除了正常情况（指堆栈，寄存器没有被非法修改）下这种情况是如何发生的,所以没有很多思路.


